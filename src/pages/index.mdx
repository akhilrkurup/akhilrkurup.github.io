---
layout: ../layouts/Layout.astro
title: SEAL- Safe and Efficient Abstraction Learning for Robotic Planning
description: Simple project page template for your research paper, built with Astro and Tailwind CSS
favicon: converted.svg
thumbnail: screenshot-light.png
---

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import { ImageComparison } from "../components/ImageComparison.tsx";


import naive from "../assets/naive.mp4"
import with_replanning from "../assets/with_replanning.mp4"
import draw_final from "../assets/intro.png"
import robot_exp from "../assets/robot_exp.png"
import comparison from "../assets/comparison.jpg"



import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Akhil R Kurup",
      institution: "Indian Indian Institute of Science Bangalore",
    },
    {
      name: "Ravi Prakash",
      institution: "Indian Institute of Science Bangalore",
    },

  ]}
  />

<Figure>
  <Image slot="figure" source={draw_final} altText="Diagram illustrating our framework" />
  <span slot="caption">SEAL Framework: We start with a set of expert demonstrations of a trajectory segmented into
the different semantic modes each part of the trajectory represents. A classifier is trained on these
using the per-point labels. Using the uncertainty over predictions of the classifier, we select uncertain
points and collect a few more expert demonstrations passing through those points. Finally we train
a classifier on the combined set of trajectories which can then be used for downstream tasks..</span>
</Figure>

<HighlightedSection>

## Abstract

Advances in Large Language Models have enabled the decomposition of complex tasks into subtasks for easier planning.
A key challenge remains: how can we ensure a robot's task execution respects implicit task constraints and remains robust to real-world perturbations. 
We propose an active learning framework to efficiently learn a symbolic mode classifier that maps robot states to discrete modes, thereby establishing a grounded world model. 
By using active learning to iteratively improve our classification by querying experts at uncertain mode boundaries, we can match the performance of previous approaches but without any potentially unsafe perturbed trajectories or oracles, thereby enabling policies to become robust to task-level disturbances. 
Our findings underscore the potential for data-efficient, safe, and reliable abstraction learning to support long-term robot autonomy.


</HighlightedSection>

## Results

<Figure>
  <Image slot="figure" source={comparison} altText="Comaprison of different approaches" />
  <span slot="caption">Comparison of the classification performance of the different methods.(b) uses 6+6 demos, (c) uses only the original 6 demos and (d) uses 6 demos + 1000 perturbations.</span>
</Figure>

| Method | IoU |
|:---|---:|
| SEAL | 0.82 |
| GLiDE(1000) | 0.8 |
| SEAL (Naive) | 0.73 |
| GLiDE(400) | 0.63 |
| GLiDE(100) | 0.53 |

This learned mode classifier enables us to replan after perturbations.

<TwoColumns>
  <Figure slot="left">
    <Video slot="figure" source={naive} />
    <span slot="caption">The policy fails to respect mode sequence after perturbations.</span>
  </Figure>
  <Figure slot="right">
    <Video slot="figure" source={with_replanning} />
    <span slot="caption">The mode classifier helps in relplanning after perturbations. Since mode boundaries are also known we can plan around them as well.</span>
  </Figure>
</TwoColumns>


We also trained a mode classifier on demonstrations of a inspection task. The aim is to move in the order shown so that the item can be succesfully scanned.

<Figure>
  <Image slot="figure" source={robot_exp} altText="Real World robot experiment" />
  <span slot="caption">Setup of the inspection task. (a) shows the real world setup.(b) shows the trajectories collected after re querying. (c) shows the predictions of the learned mode classifier.</span>
</Figure>


